import logging
from pprint import pformat
from typing import List, Dict

from AiOrchestration.ChatGptWrapper import ChatGptWrapper, ChatGptRole
from AiOrchestration.ChatGptModel import ChatGptModel
from Data.FileManagement import FileManagement
from Utilities.Decorators import handle_errors
from Utilities.ErrorHandler import ErrorHandler
from Utilities.Utility import Utility


class AiOrchestrator:
    """Manages interactions with a given large language model (LLM), specifically designed for processing user input and
    generating appropriate responses.
    """

    def __init__(self, input_files: List[str] = None):
        """Initializes a llm wrapper instance that can make call to a given model (currently only OpenAi models).
        ToDo: if you remove input files, other classes that use the executor no longer have to be singletons

        :param input_files: A list of file names that users provide for analysis or reference in generating responses
        """
        self.input_files = input_files or []
        self.prompter = ChatGptWrapper()

        ErrorHandler.setup_logging()

    @handle_errors(raise_errors=True)
    def execute(
        self,
        system_prompts: List[str] | str,
        user_prompts: List[str] | str,
        rerun_count: int = 1,
        judgement_criteria: List[str] = None,
        model: ChatGptModel = ChatGptModel.CHAT_GPT_4_OMNI_MINI,
        assistant_messages: List[str] = None,
        streaming: bool = False
    ) -> str:
        """Generate a response based on system and user prompts.

        This method constructs messages to be sent to the OpenAI API and retrieves a response.

        :param system_prompts: Messages providing contextual guidance to the LLM
        :param user_prompts: The user prompts representing instructions
        :param rerun_count: number of times to rerun outputs
        :param judgement_criteria: Criteria for evaluating multiple output responses
        :param model: Preferred LLM model to be used
        :param assistant_messages: History for the current interaction
        :param streaming: whether a streaming response should be triggered
        :return: The response generated by OpenAI or an error message
        """
        assistant_messages = assistant_messages or []
        messages = self.generate_messages(system_prompts, user_prompts, assistant_messages, model)
        logging.info(f"Executing LLM call with messages: \n{pformat(messages, width=500)}")

        if streaming:
            logging.info("Executing streaming response")
            response = Utility.execute_with_retries(
                lambda: self.prompter.get_open_ai_streaming_response(messages, model)
            )
        else:
            logging.info("Executing non-streaming response")
            response = self._handle_rerun(messages, model, rerun_count, judgement_criteria)

        if not response:
            logging.error("No response from OpenAI API.")
            raise Exception("Failed to get response from OpenAI API.")

        logging.info(f"Executor Task Finished, with response:\n{response}")
        return response

    def _handle_rerun(
            self,
            messages: List[Dict[str, str]],
            model: ChatGptModel,
            rerun_count: int,
            judgement_criteria: List[str]
    ) -> str:
        if rerun_count == 1:
            return Utility.execute_with_retries(
                lambda: self.prompter.get_open_ai_response(messages, model)
            )

        responses = Utility.execute_with_retries(
            lambda: self.prompter.get_open_ai_response(messages, model, rerun_count=rerun_count)
        )
        logging.info(f"Messages({rerun_count}):\n{responses}")

        new_messages = self.generate_messages("", judgement_criteria, responses, model)
        return Utility.execute_with_retries(
            lambda: self.prompter.get_open_ai_response(new_messages, model)
        )

    @handle_errors(raise_errors=True)
    def execute_function(
        self,
        system_prompts: List[str] | str,
        user_prompts: List[str] | str,
        function_schema: str,
        model: ChatGptModel = ChatGptModel.CHAT_GPT_4_OMNI_MINI
    ) -> Dict[str, object]:
        """Generates a structured response based on system and user prompts.

        :param system_prompts: The system prompts to guide the thinking process
        :param user_prompts: The specific task to address
        :param function_schema: the specified function schema to output
        :param model: Preferred llm model to be used
        :return: The response generated by OpenAI or an error message
        """
        if not function_schema:
            logging.error("No function schema found")
            raise ValueError("Function schema cannot be empty.")

        messages = self.generate_messages(system_prompts, user_prompts, model=model)
        response = Utility.execute_with_retries(
            lambda: self.prompter.get_open_ai_function_response(messages, function_schema, model)
        )

        if response is None:
            logging.error("Failed to obtain a valid response from OpenAI API.")
            raise RuntimeError("OpenAI API returned no response.")

        logging.info(f"Function evaluated with response: {pformat(response)}")
        return response

    def generate_messages(
        self,
        system_prompts: List[str] | str,
        user_prompts: List[str],
        assistant_messages: List[str] = None,
        model: ChatGptModel = ChatGptModel.CHAT_GPT_4_OMNI_MINI
    ) -> List[Dict[str, str]]:
        """Generates the list of messages by composing user and system prompts.

        :param system_prompts: Messages providing contextual guidance to the LLM
        :param user_prompts: User prompts representing instructions
        :param assistant_messages: History of the current interaction
        :param model: The selected model
        :return: List of formatted messages for LLM processing
        """
        logging.debug(f"Composing messages with user prompts: {pformat(user_prompts, width=280)}")

        system_prompts = Utility.ensure_string_list(system_prompts)
        user_prompts = Utility.ensure_string_list(user_prompts)
        if assistant_messages:
            assistant_messages = Utility.ensure_string_list(assistant_messages)

        messages = self.build_role_messages(system_prompts, user_prompts, assistant_messages, model)
        logging.debug(f"Messages: {pformat(messages)}")
        logging.info(f"Tokens used (limit 128k): {Utility.calculate_tokens_used(messages)}")
        return messages

    def build_role_messages(
        self,
        system_prompts: List[str],
        user_prompts: List[str],
        assistant_messages: List[str] = None,
        model: ChatGptModel = ChatGptModel.CHAT_GPT_4_OMNI_MINI
    ) -> List[Dict[str, str]]:
        """Creates a list of messages to be handled by the ChatGpt API, the most important messages is the very last
        'latest' message in the list

        :param system_prompts: list of instructions to be saved as system info
        :param user_prompts: primary initial instruction and other supplied material
        :param assistant_messages: messages which represent the prior course of the conversation
        :param model: The selected model, defaulting to the economical ChatGpt 4o mini. o1 models will trigger a
         reformat of system_prompts into user_prompts
        :return:
        """
        role_messages = []
        if assistant_messages:
            role_messages.extend({
                "role": ChatGptRole.ASSISTANT.value,
                "content": prompt
            } for prompt in assistant_messages)

        logging.info(f"Number of input files: {self.input_files}")
        for file in self.input_files:
            try:
                content = FileManagement.read_file(file)
                role_messages.append({"role": ChatGptRole.USER.value, "content": f"<{file}>{content}</{file}>"})
            except FileNotFoundError:
                logging.error(f"File not found: {file}. Please ensure the file exists.")
                role_messages.append({"role": ChatGptRole.SYSTEM.value, "content": f"File not found: {file}"})
            except Exception as e:
                logging.error(f"Error reading file {file}: {e}")
                role_messages.append(
                    {"role": ChatGptRole.SYSTEM.value, "content": f"Error reading file {file}. Exception: {e}"}
                )

        role_messages += [
            {"role": ChatGptRole.SYSTEM.value, "content": prompt} for prompt in system_prompts
        ] + [
            {"role": ChatGptRole.USER.value, "content": prompt} for prompt in user_prompts
        ]

        if model == ChatGptModel.CHAT_GPT_O1_MINI or model == ChatGptModel.CHAT_GPT_O1_PREVIEW:
            role_messages = self._handle_o1_model_messages(role_messages)

        logging.info(f"Generated role messages - [{model}] : {role_messages}")

        return role_messages

    @staticmethod
    def _handle_o1_model_messages(role_messages: List[Dict[str, str]]):
        """
        For the time being 01 models are in beta and cannot process system prompts
        (I honestly have no idea how that's remotely possible but whatever,
        I'm not the one being paid $150,000+ to muck about)
        """
        non_system_messages = [
            role_message for role_message in role_messages if
            role_message.get("role") != ChatGptRole.SYSTEM.value
        ]

        system_messages = [
            {**role_message, "role": ChatGptRole.USER.value}
            for role_message in role_messages
            if role_message.get("role") == ChatGptRole.SYSTEM.value
        ]
        # Reassemble the list with system messages first, i.e. first in the message log history
        role_messages = system_messages + non_system_messages
        return role_messages


if __name__ == '__main__':
    ai_wrapper = AiOrchestrator(["example.txt"])

    print(ai_wrapper.execute(
        ["""Given the following user prompt what are your questions, be concise and targeted.
        What would you like to know before proceeding"""],
        ["rewrite solution.txt to be more concise"],
        model=ChatGptModel.CHAT_GPT_O1_MINI
    ))
