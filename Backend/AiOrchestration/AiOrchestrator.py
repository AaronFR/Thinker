import logging
from typing import List, Dict, Union

from AiOrchestration.ChatGptModel import ChatGptModel
from AiOrchestration.GeminiModel import GeminiModel
from AiOrchestration.ChatGptMessageBuilder import generate_messages
from Constants.Exceptions import AI_RESOURCE_FAILURE, FUNCTION_SCHEMA_EMPTY, NO_RESPONSE_OPEN_AI_API

from Utilities.Decorators import handle_errors, specify_functionality_context
from Utilities.ErrorHandler import ErrorHandler
from Utilities.Utility import Utility
from Utilities.models import determine_prompter


class AiOrchestrator:
    """
    Manages interactions with a given large language model (LLM), specifically designed for processing user input and
    generating appropriate responses.
    """
    # ToDo Re-implement differentiated re-runs
    RERUN_SYSTEM_MESSAGES = [
        "",  # First rerun: No additions
        "prioritize coherency",
        "prioritize creativity",
        "prioritize intelligent and well thought out solutions",
        "prioritize thinking out your response FIRST and responding LAST",
        "prioritize non-linear thinking, utilise niche deductions to make an unusual but possibly insightful solution"
    ]
    chat_gpt_models = {model.value for model in ChatGptModel}
    gemini_models = {model.value for model in GeminiModel}

    def __init__(self):
        """
        Initializes a llm wrapper instance that can make call to a given model (currently only OpenAi models).
        """
        self.prompter = None

        ErrorHandler.setup_logging()

    @handle_errors(raise_errors=True)
    def execute(
        self,
        system_prompts: List[str] | str,
        user_prompts: List[str] | str,
        rerun_count: int = 1,
        judgement_criteria: List[str] = None,
        model: Union[ChatGptModel, GeminiModel] = ChatGptModel.CHAT_GPT_4_OMNI_MINI,
        assistant_messages: List[str] = None,
        streaming: bool = False
    ) -> str:
        """
        Generate a response based on system and user prompts.

        This method constructs messages to be sent to the OpenAI API and retrieves a response.

        :param system_prompts: Messages providing contextual guidance to the LLM
        :param user_prompts: The user prompts representing instructions
        :param rerun_count: number of times to rerun outputs
        :param judgement_criteria: Criteria for evaluating multiple output responses
        :param model: Preferred LLM model to be used
        :param assistant_messages: History for the current interaction
        :param streaming: whether a streaming response should be triggered
        :return: The response generated by OpenAI or an error message
        """
        self.prompter = determine_prompter(model.value)

        assistant_messages = assistant_messages or []
        messages = generate_messages(system_prompts, user_prompts, assistant_messages, model)
        logging.info(f"Executing LLM call with messages: \n{messages}")

        response = self._handle_rerun(messages, model, rerun_count, judgement_criteria, streaming)

        if not response:
            logging.error(f"No response from AI API : {self.prompter}")
            raise Exception(AI_RESOURCE_FAILURE)

        logging.info(f"Executor Task Finished, with response:\n{response}")
        return response

    def _handle_rerun(
            self,
            messages: List[Dict[str, str]],
            model: ChatGptModel,
            rerun_count: int,
            judgement_criteria: List[str],
            streaming: bool = False
    ) -> str:
        logging.info("EXECUTING PROMPT")
        if rerun_count == 1:
            return Utility.execute_with_retries(
                lambda: self.prompter.get_ai_streaming_response(messages, model) if streaming
                else self.prompter.get_ai_response(messages, model)
            )

        if rerun_count > 1:
            logging.info("Parallel reruns are enabled. Executing non-differentiated parallel rerun logic.")
            return self._handle_reruns(messages, model, rerun_count, judgement_criteria, streaming)

    @specify_functionality_context("best_of")
    def _handle_reruns(
            self,
            messages: List[Dict[str, str]],
            model: ChatGptModel,
            rerun_count: int,
            judgement_criteria: List[str],
            streaming: bool = False
    ) -> str:
        responses = Utility.execute_with_retries(
            lambda: self.prompter.get_ai_response(messages, model, rerun_count=rerun_count)
        )
        logging.info(f"Re-run responses ({rerun_count}) : \n{responses}")

        new_messages = generate_messages("", judgement_criteria, responses, model)

        return Utility.execute_with_retries(
            lambda: self.prompter.get_ai_streaming_response(new_messages, model) if streaming
            else self.prompter.get_ai_response(new_messages, model)
        )

    @handle_errors(raise_errors=True)
    def execute_function(
        self,
        system_prompts: List[str] | str,
        user_prompts: List[str] | str,
        function_schema: str,
        model: ChatGptModel = ChatGptModel.CHAT_GPT_4_OMNI_MINI
    ) -> Dict[str, object]:
        """
        Generates a structured response based on system and user prompts.

        ToDo: I am not a fan of function calls large schema inputs, run tests to check if a well put system prompt and
        interpreter alone can replace the role of the remaining function calls

        :param system_prompts: The system prompts to guide the thinking process
        :param user_prompts: The specific task to address
        :param function_schema: the specified function schema to output
        :param model: Preferred llm model to be used
        :return: The response generated by OpenAI or an error message
        """
        self.prompter = determine_prompter(model.value)

        if not function_schema:
            logging.error("No function schema found")
            raise ValueError(FUNCTION_SCHEMA_EMPTY)

        messages = generate_messages(system_prompts, user_prompts, model=model)
        response = Utility.execute_with_retries(
            lambda: self.prompter.get_ai_function_response(messages, function_schema, model)
        )

        if response is None:
            logging.error("Failed to obtain a valid response from OpenAI API.")
            raise RuntimeError(NO_RESPONSE_OPEN_AI_API)

        logging.info(f"Function evaluated with response: {response}")
        return response


if __name__ == '__main__':
    ai_wrapper = AiOrchestrator()

    print(ai_wrapper.execute(
        ["""Given the following user prompt what are your questions, be concise and targeted.
        What would you like to know before proceeding"""],
        ["rewrite solution.txt to be more concise"],
        model=ChatGptModel.CHAT_GPT_O1_MINI
    ))
