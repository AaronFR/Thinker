import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict

from flask import copy_current_request_context, current_app

from AiOrchestration.ChatGptWrapper import ChatGptWrapper
from AiOrchestration.ChatGptModel import ChatGptModel
from AiOrchestration.ChatGptMessageBuilder import generate_messages
from Data.Configuration import Configuration
from Utilities.Decorators import handle_errors
from Utilities.ErrorHandler import ErrorHandler
from Utilities.Utility import Utility


class AiOrchestrator:
    """Manages interactions with a given large language model (LLM), specifically designed for processing user input and
    generating appropriate responses.
    """
    RERUN_SYSTEM_MESSAGES = [
        "",  # First rerun: No additions
        "prioritize coherency",
        "prioritize creativity",
        "prioritize intelligent and well thought out solutions",
        "prioritize thinking out your response FIRST and responding LAST",
        "prioritize non-linear thinking, utilise niche deductions to make an unusual but possibly insightful solution"
    ]

    def __init__(self):
        """
        Initializes a llm wrapper instance that can make call to a given model (currently only OpenAi models).
        """
        self.prompter = ChatGptWrapper()

        ErrorHandler.setup_logging()

    @handle_errors(raise_errors=True)
    def execute(
        self,
        system_prompts: List[str] | str,
        user_prompts: List[str] | str,
        rerun_count: int = 1,
        judgement_criteria: List[str] = None,
        model: ChatGptModel = ChatGptModel.CHAT_GPT_4_OMNI_MINI,
        assistant_messages: List[str] = None,
        streaming: bool = False
    ) -> str:
        """Generate a response based on system and user prompts.

        This method constructs messages to be sent to the OpenAI API and retrieves a response.

        :param system_prompts: Messages providing contextual guidance to the LLM
        :param user_prompts: The user prompts representing instructions
        :param rerun_count: number of times to rerun outputs
        :param judgement_criteria: Criteria for evaluating multiple output responses
        :param model: Preferred LLM model to be used
        :param assistant_messages: History for the current interaction
        :param streaming: whether a streaming response should be triggered
        :return: The response generated by OpenAI or an error message
        """
        assistant_messages = assistant_messages or []
        messages = generate_messages(system_prompts, user_prompts, assistant_messages, model)
        logging.info(f"Executing LLM call with messages: \n{messages}")

        response = self._handle_rerun(messages, model, rerun_count, judgement_criteria, streaming)

        if not response:
            logging.error("No response from OpenAI API.")
            raise Exception("Failed to get response from OpenAI API.")

        logging.info(f"Executor Task Finished, with response:\n{response}")
        return response

    def _handle_rerun(
            self,
            messages: List[Dict[str, str]],
            model: ChatGptModel,
            rerun_count: int,
            judgement_criteria: List[str],
            streaming: bool = False
    ) -> str:
        logging.info("ðŸ¤”... Executing Prompt")
        if rerun_count == 1:
            return Utility.execute_with_retries(
                lambda: self.prompter.get_open_ai_streaming_response(messages, model) if streaming
                else self.prompter.get_open_ai_response(messages, model)
            )

        if rerun_count > 1:
            logging.info("Parallel reruns are enabled. Executing non-differentiated parallel rerun logic.")
            return self._handle_reruns(messages, model, rerun_count, judgement_criteria, streaming)

    def _handle_reruns(
            self,
            messages: List[Dict[str, str]],
            model: ChatGptModel,
            rerun_count: int,
            judgement_criteria: List[str],
            streaming: bool = False
    ) -> str:
        responses = Utility.execute_with_retries(
            lambda: self.prompter.get_open_ai_response(messages, model, rerun_count=rerun_count)
        )
        logging.info(f"Re-run responses ({rerun_count}) : \n{responses}")

        new_messages = generate_messages("", judgement_criteria, responses, model)

        return Utility.execute_with_retries(
            lambda: self.prompter.get_open_ai_streaming_response(new_messages, model) if streaming
            else self.prompter.get_open_ai_response(new_messages, model)
        )

    @handle_errors(raise_errors=True)
    def execute_function(
        self,
        system_prompts: List[str] | str,
        user_prompts: List[str] | str,
        function_schema: str,
        model: ChatGptModel = ChatGptModel.CHAT_GPT_4_OMNI_MINI
    ) -> Dict[str, object]:
        """Generates a structured response based on system and user prompts.

        :param system_prompts: The system prompts to guide the thinking process
        :param user_prompts: The specific task to address
        :param function_schema: the specified function schema to output
        :param model: Preferred llm model to be used
        :return: The response generated by OpenAI or an error message
        """
        if not function_schema:
            logging.error("No function schema found")
            raise ValueError("Function schema cannot be empty.")

        messages = generate_messages(system_prompts, user_prompts, model=model)
        response = Utility.execute_with_retries(
            lambda: self.prompter.get_open_ai_function_response(messages, function_schema, model)
        )

        if response is None:
            logging.error("Failed to obtain a valid response from OpenAI API.")
            raise RuntimeError("OpenAI API returned no response.")

        logging.info(f"Function evaluated with response: {response}")
        return response


if __name__ == '__main__':
    ai_wrapper = AiOrchestrator()

    print(ai_wrapper.execute(
        ["""Given the following user prompt what are your questions, be concise and targeted.
        What would you like to know before proceeding"""],
        ["rewrite solution.txt to be more concise"],
        model=ChatGptModel.CHAT_GPT_O1_MINI
    ))
