import logging
from typing import List, Dict, Optional

from deprecated.classic import deprecated

from AiOrchestration.AiModel import AiModel
from AiOrchestration.ChatGptModel import ChatGptModel
from AiOrchestration.GeminiModel import GeminiModel
from AiOrchestration.ChatGptMessageBuilder import generate_messages
from Constants.Exceptions import AI_RESOURCE_FAILURE, FUNCTION_SCHEMA_EMPTY, NO_RESPONSE_OPEN_AI_API
from Data.Configuration import Configuration
from Utilities.Contexts import get_user_configuration

from Utilities.Decorators import handle_errors
from Utilities.PaymentDecorators import specify_functionality_context
from Utilities.ErrorHandler import ErrorHandler
from Utilities.Utility import Utility
from Utilities.models import determine_prompter, find_model_enum_value


class AiOrchestrator:
    """
    Manages interactions with a given large language model (LLM), designed for processing user input and
    generating appropriate responses.
    """
    # ToDo Re-implement differentiated re-runs
    RERUN_SYSTEM_MESSAGES = [
        "",  # First rerun: No additions
        "prioritize coherency",
        "prioritize creativity",
        "prioritize intelligent and well thought out solutions",
        "prioritize thinking out your response FIRST and responding LAST",
        "prioritize non-linear thinking, utilise niche deductions to make an unusual but possibly insightful solution"
    ]
    chat_gpt_models = {model.value for model in ChatGptModel}
    gemini_models = {model.value for model in GeminiModel}

    def __init__(self):
        """
        Initializes an LLM wrapper instance that can call a given supported model.
        """
        self.prompter = None
        self.default_background_model = ChatGptModel.CHAT_GPT_4_OMNI_MINI

        ErrorHandler.setup_logging()

    @handle_errors(raise_errors=True)
    def execute(
        self,
        system_prompts: List[str] | str,
        user_prompts: List[str] | str,
        rerun_count: int = 1,
        judgement_criteria: Optional[List[str]] = None,
        model: Optional[AiModel] = None,
        assistant_messages: Optional[List[str]] = None,
        streaming: bool = False
    ) -> str:
        """
        Generate a response based on system and user prompts.

        :param system_prompts: Messages providing contextual guidance to the LLM
        :param user_prompts: The user prompts representing instructions
        :param rerun_count: number of times to rerun outputs
        :param judgement_criteria: Criteria for evaluating multiple output responses
        :param model: The preferred LLM model; if not provided, the default is used.
        :param assistant_messages: History of previous assistant messages.
        :param streaming: Whether to return a streaming response.
        :return: The response generated by OpenAI or an error message
        :raises Exception: If no response is received from the AI API.
        """
        if not model:
            config = Configuration.load_config()
            default_model_string = config.get('models', {}).get(
                "default_background_model",
                ChatGptModel.CHAT_GPT_4_OMNI_MINI.value
            )
            model = find_model_enum_value(default_model_string)
        self.prompter = determine_prompter(model.value)

        assistant_messages = assistant_messages or []
        messages = generate_messages(system_prompts, user_prompts, assistant_messages, model)

        logging.info(f"Executing LLM call with messages:\n{messages}")
        response = self._handle_rerun(messages, model, rerun_count, judgement_criteria, streaming)

        if not response:
            logging.error(f"No response from AI API : {self.prompter}")
            raise Exception(AI_RESOURCE_FAILURE)

        logging.info(f"Execution finished with response:\n{response}")
        return response

    def _handle_rerun(
        self,
        messages: List[Dict[str, str]],
        model: AiModel,
        rerun_count: int,
        judgement_criteria: Optional[List[str]],
        streaming: bool = False
    ) -> str:
        """
        Handles a single or multiple reruns of the API call.
        """
        logging.info("EXECUTING PROMPT")
        if rerun_count == 1:
            return Utility.execute_with_retries(
                lambda: self.prompter.get_ai_streaming_response(messages, model) if streaming
                else self.prompter.get_ai_response(messages, model)
            )

        if rerun_count > 1:
            logging.info("Parallel reruns are enabled. Executing non-differentiated parallel rerun logic.")
            return self._handle_reruns(messages, model, rerun_count, judgement_criteria, streaming)

    @specify_functionality_context("best_of")
    def _handle_reruns(
        self,
        messages: List[Dict[str, str]],
        model: AiModel,
        rerun_count: int,
        judgement_criteria: Optional[List[str]],
        streaming: bool = False
    ) -> str:
        responses = Utility.execute_with_retries(
            lambda: self.prompter.get_ai_response(messages, model, rerun_count=rerun_count)
        )
        logging.info(f"Re-run responses ({rerun_count}) : \n{responses}")

        # Generate new messages including the judgement criteria and the responses from reruns.
        new_messages = generate_messages("", judgement_criteria, responses, model)

        return Utility.execute_with_retries(
            lambda: self.prompter.get_ai_streaming_response(new_messages, model) if streaming
            else self.prompter.get_ai_response(new_messages, model)
        )

    @deprecated
    @handle_errors(raise_errors=True)
    def execute_function(
        self,
        system_prompts: List[str] | str,
        user_prompts: List[str] | str,
        function_schema: str,
        model: AiModel = ChatGptModel.CHAT_GPT_4_OMNI_MINI
    ) -> Dict[str, object]:
        """
        :param user_prompts: The specific task to address
        Generates a structured response based on the provided prompts and function schema.

        :param system_prompts: System messages guiding the task.
        :param function_schema: The schema that defines the structure of the expected function output.
        :param model: The LLM model that will be used to execute the function
        :return: The generated structured response as a dictionary.
        :raises ValueError: If no function schema is provided.
        :raises RuntimeError: If no valid response is obtained from the API.
        """
        self.prompter = determine_prompter(model.value)

        if not function_schema:
            logging.error("No function schema provided.")
            raise ValueError(FUNCTION_SCHEMA_EMPTY)

        messages = generate_messages(system_prompts, user_prompts, model=model)
        response = Utility.execute_with_retries(
            lambda: self.prompter.get_ai_function_response(messages, function_schema, model)
        )

        if response is None:
            logging.error("Failed to obtain a valid response from OpenAI API.")
            raise RuntimeError(NO_RESPONSE_OPEN_AI_API)

        logging.info(f"Function evaluated with response: {response}")
        return response


if __name__ == '__main__':
    try:
        ai_wrapper = AiOrchestrator()
        result = ai_wrapper.execute(
            system_prompts=[
                "Given the following user prompt, what are your questions? Be concise and targeted. "
                "What would you like to know before proceeding?"
            ],
            user_prompts=["rewrite solution.txt to be more concise"],
            model=ChatGptModel.CHAT_GPT_O1_MINI
        )
        print(result)
    except Exception as e:
        logging.error(f"An error occurred: {e}")
        print(f"An error occurred: {e}")
