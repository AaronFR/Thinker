import enum
import logging
import time
from pprint import pformat
from typing import List, Dict
from openai import OpenAI, OpenAIError
import Constants
import Prompter
from ThoughtProcessor.FileManagement import FileManagement
from Utility import Utility


class Role(enum.Enum):
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
    TOOL = "tool"


class Thought:
    """Class to manage and process 'thoughts' in the Thinker system using the OpenAI API"""

    def __init__(self, input_files: List[str], prompter: Prompter.Prompter, open_ai_client: OpenAI):
        """Each 'Thought' represents a single call to the api that then adds to the existing body of files representing
        a solution to the initial task that triggered the thought process.

        :param input_files: file references representing files passed in by the user for processing/reference
        :param prompter: handles prompting the OpenAi API
        :param open_ai_client: The OpenAI API client instance.
        """
        self.prompter = prompter
        self.open_ai_client = open_ai_client
        self.input_files = input_files

    def think(self, system_prompts: List[str], user_prompt: str) -> str:
        """Generate a response based on system and user prompts.
        ToDo: At some point actions other than writing will be needed, e.g. 'web search'
        ToDo: With large context lengths approx 5k+ the current executive prompt can fail to produce an actual json output and get confused into writing a unironic answer
        #Solved if executive files only review summaries of input files

        :param system_prompts: The system prompts to guide the thinking process.
        :param user_prompt: The task the thought process is to be dedicated to.
        :return: The response generated by OpenAI or an error message.
        """
        logging.info(f"Thinking...{user_prompt}")
        messages = self.generate_role_messages(system_prompts, user_prompt)

        logging.debug(f"Messages: {messages}")
        logging.info(f"Tokens used (limit 128k): {Utility.calculate_tokens_used(messages)}")

        response = Utility.execute_with_retries(lambda: self.get_open_ai_response(messages))
        if not response:
            logging.error("No response from OpenAI API.")
            raise

        logging.info(f"Thought finished")
        return response

    def generate_role_messages(self, system_prompts: List[str], user_prompt: str):
        """
        ToDo: generalise for user_prompt***S***
        :param system_prompts:
        :param user_prompt:
        :return:
        """
        role_messages = []

        # Add input file contents as user messages
        for file in self.input_files:
            try:
                content = FileManagement.read_file(file)
                role_messages.append((Role.USER, f"{file}: \n{content}"))
            except FileNotFoundError:
                logging.error(f"File not found: {file}. Please ensure the file exists.")
                role_messages.append((Role.USER, f"File not found: {file}"))
            except Exception as e:
                logging.error(f"Error reading file {file}: {e}")
                role_messages.append((Role.USER, f"Error reading file {file}. Exception: {e}"))

        # Add the user prompt
        role_messages.append((Role.USER, user_prompt))

        # Prepare system messages as role tuples
        system_role_messages = [(Role.SYSTEM, prompt) for prompt in system_prompts]

        return self.create_messages_by_role(role_messages + system_role_messages)

    @staticmethod
    def generate_message(role: Role, content: str) -> Dict[str, str]:
        """Generate a message based on the specified role.

        :param role: The role of the message sender.
        :param content: The content of the message to be sent.
        :return: A dictionary representing the message.
        """
        return {"role": role.value, "content": content}

    @staticmethod
    def create_messages_by_role(role_messages: List[tuple]) -> List[Dict[str, str]]:
        """Create messages from a list of tuples containing role and content.

        :param role_messages: A list of tuples with each tuple containing a Role and its message content.
        :return: A list of dictionaries representing user messages.
        """
        messages = []
        for role, content in role_messages:
            messages.append(Thought.generate_message(role, content))
        return messages

    def get_open_ai_response(self, messages: List[dict]) -> str | None:
        """Request a response from the OpenAI API.

        :param messages: The system and user messages to send to the ChatGpt client
        :return: The content of the response from OpenAI or an error message to inform the next Thought.
        """
        try:
            logging.debug(f"Calling OpenAI API with messages: {messages}")
            response = self.open_ai_client.chat.completions.create(
                model=Constants.MODEL_NAME, messages=messages
            ).choices[0].message.content
            return response or "[ERROR: NO RESPONSE FROM OpenAI API]"
        except OpenAIError as e:
            logging.error(f"OpenAI API")
            raise
        except Exception as e:
            logging.error(f"Unexpected error")
            raise


if __name__ == '__main__':
    prompter = Prompter.Prompter()
    openai = OpenAI()
    thought = Thought(["solution.txt"], prompter, openai)

    print(thought.think(
        Constants.EXECUTIVE_PROMPT,
        """
        Give me a history of India"""))
        #"How can the Thought.py be improved? Write an improved version"))
